{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2760,"status":"ok","timestamp":1625343937339,"user":{"displayName":"meysam asgari chenaghlu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN-uRJ5KciT9X_sZpZ4pOpEANlj5GCtXuY2ecq=s64","userId":"13491140841572947804"},"user_tz":-180},"id":"nW3mGrSWBsbv","outputId":"e6d25990-76e6-40e3-821c-ed966dcb285f"},"outputs":[],"source":["# !pip install transformers[sentencepiece]"]},{"cell_type":"markdown","metadata":{"id":"YtDAWQJnBegn"},"source":["## BART zero-shot Text Classification"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"elapsed":11836,"status":"ok","timestamp":1625343952712,"user":{"displayName":"meysam asgari chenaghlu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN-uRJ5KciT9X_sZpZ4pOpEANlj5GCtXuY2ecq=s64","userId":"13491140841572947804"},"user_tz":-180},"id":"4HZPHyA-CEsb","outputId":"673b2271-0ce4-4554-eb81-a865420890f3"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sequence</th>\n","      <th>labels</th>\n","      <th>scores</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>one day I will see the world</td>\n","      <td>travel</td>\n","      <td>0.795757</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>one day I will see the world</td>\n","      <td>exploration</td>\n","      <td>0.199331</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>one day I will see the world</td>\n","      <td>dancing</td>\n","      <td>0.002621</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>one day I will see the world</td>\n","      <td>cooking</td>\n","      <td>0.002291</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                       sequence       labels    scores\n","0  one day I will see the world       travel  0.795757\n","1  one day I will see the world  exploration  0.199331\n","2  one day I will see the world      dancing  0.002621\n","3  one day I will see the world      cooking  0.002291"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline \n","import pandas as pd \n","classifier = pipeline(\"zero-shot-classification\", \n","                      model=\"facebook/bart-large-mnli\") \n","sequence_to_classify = \"one day I will see the world\" \n","candidate_labels = ['travel', \n","                    'cooking', \n","                    'dancing', \n","                    'exploration'] \n","result = classifier(sequence_to_classify, candidate_labels) \n","pd.DataFrame(result)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"elapsed":1343,"status":"ok","timestamp":1625343956133,"user":{"displayName":"meysam asgari chenaghlu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN-uRJ5KciT9X_sZpZ4pOpEANlj5GCtXuY2ecq=s64","userId":"13491140841572947804"},"user_tz":-180},"id":"GnRsPkJOCJYF","outputId":"6193593c-79f2-402c-db07-613b532387e3"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sequence</th>\n","      <th>labels</th>\n","      <th>scores</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>one day I will see the world</td>\n","      <td>travel</td>\n","      <td>0.994511</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>one day I will see the world</td>\n","      <td>exploration</td>\n","      <td>0.938388</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>one day I will see the world</td>\n","      <td>dancing</td>\n","      <td>0.005706</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>one day I will see the world</td>\n","      <td>cooking</td>\n","      <td>0.001819</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                       sequence       labels    scores\n","0  one day I will see the world       travel  0.994511\n","1  one day I will see the world  exploration  0.938388\n","2  one day I will see the world      dancing  0.005706\n","3  one day I will see the world      cooking  0.001819"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["result = classifier(sequence_to_classify,  \n","                      candidate_labels,  \n","                      multi_label=True) \n","pd.DataFrame(result) "]},{"cell_type":"markdown","metadata":{"id":"VCfmUiUgCT6d"},"source":["## BART no pipeline\n","BART is fine-tuned on Natural Language Inference (NLI) datasets such as MNLI. These datasets contain sentence pairs and three classes for each pair; that is, *Neutral*, *Entailment*, and *Contradiction*. Models that have been trained on these datasets can capture the semantics of two sentences and classify them by assigning a label in one-hot format.   \n","If you take out Neutral labels and only use *Entailment* and *Contradiction* as your output labels, if two sentences can come after each other, then it means these two are closely related to each other. In other words, you can change the first sentence to the label (travel, for example) and the second sentence to the content (one day I will see the world, for example).  \n","According to this, if these two can come after each other, this means that the label and the content are semantically related. The following code example shows how to directly use the BART model without the zero-shot classification pipeline according to the preceding descriptions:"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7737,"status":"ok","timestamp":1625343968616,"user":{"displayName":"meysam asgari chenaghlu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN-uRJ5KciT9X_sZpZ4pOpEANlj5GCtXuY2ecq=s64","userId":"13491140841572947804"},"user_tz":-180},"id":"t-JOm5DFCe3_"},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer \n","\n","nli_model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large-mnli\") \n","tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-mnli\") "]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1625343969797,"user":{"displayName":"meysam asgari chenaghlu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN-uRJ5KciT9X_sZpZ4pOpEANlj5GCtXuY2ecq=s64","userId":"13491140841572947804"},"user_tz":-180},"id":"AK56LSGMCyB9"},"outputs":[],"source":["premise = \"one day I will see the world\" \n","# label = \"travel\" \n","label = \"retirement\"\n","hypothesis = f'This example is {label}.' "]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":341,"status":"ok","timestamp":1625343971408,"user":{"displayName":"meysam asgari chenaghlu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN-uRJ5KciT9X_sZpZ4pOpEANlj5GCtXuY2ecq=s64","userId":"13491140841572947804"},"user_tz":-180},"id":"CNr_tcIhC1mH","outputId":"9458677c-2568-4cff-dd35-e608bd7d0adc"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/guy/anaconda3/envs/mastrans/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2356: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n","  warnings.warn(\n"]}],"source":["x = tokenizer.encode( \n","    premise, \n","    hypothesis, \n","    return_tensors='pt', \n","    truncation_strategy='only_first') "]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[   0, 1264,  183,   38,   40,  192,    5,  232,    2,    2,  713, 1246,\n","           16, 3832,    4,    2]])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["x"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":570,"status":"ok","timestamp":1625343973798,"user":{"displayName":"meysam asgari chenaghlu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgN-uRJ5KciT9X_sZpZ4pOpEANlj5GCtXuY2ecq=s64","userId":"13491140841572947804"},"user_tz":-180},"id":"f8-zTXeHC3H5","outputId":"f61dd131-e420-4934-bb62-232937259af5"},"outputs":[{"data":{"text/plain":["tensor([[ 1.1138,  3.1326, -4.1702]], grad_fn=<AddmmBackward0>)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["logits = nli_model(x)[0] \n","logits"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"W2KPslP0C3iL"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0.0050], grad_fn=<SelectBackward0>)\n"]}],"source":["entail_contradiction_logits = logits[:,[0,2]]   # take the logits for entailment and contradiction\n","probs = entail_contradiction_logits.softmax(dim=1) \n","prob_label_is_true = probs[:,1] \n","print(prob_label_is_true) "]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNXy3agCfVva4GmxURx9kRr","name":"CH07b_Using_BART_for_zero_shot_learning.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('mastrans')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"46ac275f730ac4cd685e340db4d97ebc86af74e2e59cec90e9c32b87f57bddf9"}}},"nbformat":4,"nbformat_minor":0}
