{"cells":[{"cell_type":"markdown","metadata":{"id":"hdd1b6iEGeMp"},"source":["# Fine-Tuning with Native Pytorch \n","In this notebook we're going to fine-tune the pre-trained model without using the `Trainer` class from HuggingFace."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3423,"status":"ok","timestamp":1625397794148,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"M7_Bktl28lu5","outputId":"80e27cb8-e0ef-464f-94b3-dbc5bd491874"},"outputs":[],"source":["# !pip install transformers datasets"]},{"cell_type":"markdown","metadata":{"id":"087OXCoeN-Zf"},"source":["# One-step forward"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"BrjheWzxvQf2"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["DistilBertForSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (1): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (2): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (3): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (4): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (5): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import DistilBertForSequenceClassification\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n","model.train()"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1536,"status":"ok","timestamp":1625397809172,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"6h7TBrdyTjpJ"},"outputs":[{"name":"stderr","output_type":"stream","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n","The class this function is called from is 'DistilBertTokenizerFast'.\n"]}],"source":["from transformers import DistilBertTokenizerFast\n","tokenizer = DistilBertTokenizerFast.from_pretrained('bert-base-uncased')"]},{"cell_type":"markdown","metadata":{},"source":["Since the Trainer class organized the entire process for us, we did not deal with optimization and other training settings in the previous IMDb sentiment classification exercise. Now, we need to instantiate the optimizer ourselves. Here, we must select AdamW, which is an implementation of the Adam algorithm but with a weight decay fix. Recently, it has been shown that AdamW produces better training loss and validation loss than models trained with Adam. Hence, it is a widely used optimizer within many transformer training processes:"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1669,"status":"ok","timestamp":1625397812835,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"75afLI_FvQi7"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/guy/anaconda3/envs/mastrans/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["from transformers import AdamW\n","optimizer = AdamW(model.parameters(), lr=1e-3)"]},{"cell_type":"markdown","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1625397812836,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"IlBGG7IU6-LO"},"source":["To design the fine-tuning process from scratch, we must understand how to implement a single step forward and backpropagation. We can pass a single batch through the transformer layer and get the output, which is called forward propagation. Then, we must compute the loss using the output and ground truth label and update the model weight based on the loss. This is called backpropagation.  \n","\n","The following code receives three sentences associated with the labels in a single batch and performs forward propagation. At the end, the model automatically computes the loss:"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1625397814045,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"9NEdeBUh-fqW"},"outputs":[],"source":["# one step forward\n","import torch\n","texts= [\"this is a good example\",\"this is a bad example\",\"this is a good one\"]\n","labels= [1,0,1]\n","labels = torch.tensor(labels).unsqueeze(0)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1625397815290,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"xxwvtJOivQmw"},"outputs":[],"source":["encoding = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n","input_ids = encoding['input_ids']\n","attention_mask = encoding['attention_mask']"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1314,"status":"ok","timestamp":1625397817606,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"FLHyHE_4vQo0"},"outputs":[],"source":["outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","loss = outputs.loss\n","loss.backward()\n","optimizer.step()"]},{"cell_type":"markdown","metadata":{"executionInfo":{"elapsed":361,"status":"ok","timestamp":1625397819976,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"aMwRgtXovQrk"},"source":["as we can see below, the output contains the loss and logits.   \n","Logits need to be turned into probabilities by the softmax function in the case of classification. Otherwise, they are simply normalized for regression."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1625397820438,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"DnbXt46DZRmF","outputId":"fc397b18-ac98-4203-93e6-64ea55d50aec"},"outputs":[{"data":{"text/plain":["SequenceClassifierOutput(loss=tensor(0.7505, grad_fn=<NllLossBackward0>), logits=tensor([[ 0.0948, -0.0356],\n","        [-0.0710,  0.0105],\n","        [ 0.0463, -0.0764]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["outputs"]},{"cell_type":"markdown","metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1625397821212,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"7X2qm-O1xStv"},"source":["If we want to manually calculate the loss, we must not pass the labels to the model.  \n","\n","Due to this, the model only yields the logits and does not calculate the loss. In the following example, we are computing the cross-entropy loss manually:"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":474,"status":"ok","timestamp":1625397822597,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"SrtXFsuevQyH","outputId":"c695798e-4b18-467b-e643-eb076cd87a00"},"outputs":[{"data":{"text/plain":["tensor(0.6184, grad_fn=<NllLossBackward0>)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["#Manually calculate loss\n","from torch.nn import functional\n","labels = torch.tensor([1,0,1])\n","outputs = model(input_ids, attention_mask=attention_mask)\n","loss = functional.cross_entropy(outputs.logits, labels)\n","loss.backward()\n","optimizer.step()\n","loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":387,"status":"ok","timestamp":1625397825013,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"Oj0qMlTIxa8L","outputId":"254b6d5d-9452-40d6-f0ba-fd39e1c08444"},"outputs":[],"source":["outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1625397827149,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"YAZN5BUpxa-W"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"QHFl5T_aOJik"},"source":["## Training the model from entire dataset with Native PyTorch \n","we've learned how batch input is fed in the forward direction through the network in a single step. Now, it is time to design a loop that iterates over the entire dataset in batches to train the model with several epochs. To do so, we will start by designing the Dataset class. It is a subclass of torch.Dataset, inherits member variables and functions, and implements `__init__()` and `__getitem()__` abstract functions:"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":553,"status":"ok","timestamp":1625397829408,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"_hoE8Q7WmIK2"},"outputs":[],"source":["from torch.utils.data import Dataset\n","class MyDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","    def __len__(self):\n","        return len(self.labels)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2958,"status":"ok","timestamp":1625397835330,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"M9FCV-5oQcug","outputId":"5ea241ab-b2be-4b77-a67d-13e929ca155b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import DistilBertForSequenceClassification\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n","from transformers import BertTokenizerFast\n","tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"]},{"cell_type":"markdown","metadata":{},"source":["Let's fine-tune the model for sentiment analysis by taking another sentiment analysis dataset called the SST-2 dataset; that is, Stanford Sentiment Treebank v2 (SST2). We will also load the corresponding metric for SST-2 for evaluation, as follows:"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1675,"status":"ok","timestamp":1625397838368,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"0GZc-IP_qOPr","outputId":"cab7b85d-8e72-4b44-96e6-a0354d2ea252"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset glue/sst2 (download: 7.09 MiB, generated: 4.81 MiB, post-processed: Unknown size, total: 11.90 MiB) to /home/guy/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"512cffd7f8364fd6a27d90bd2e68701e","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/7.44M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fee938cc71e84366866498d182fd2b6d","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27d797581ac7484da8447cf74c519aad","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53d0ab25f9ce4810ade1190b022898be","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset glue downloaded and prepared to /home/guy/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"70ad136294244a419cf3f1165fafafb3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10634d01f7904f0db8dc5ed388a09343","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import datasets\n","from datasets import load_dataset\n","sst2= load_dataset(\"glue\",\"sst2\")\n","from datasets import load_metric\n","metric = load_metric(\"glue\", \"sst2\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1625397840153,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"Zf329--Kbx-f"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1625397840602,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"Q0u7JTJZePSD"},"outputs":[],"source":["texts=sst2['train']['sentence']\n","labels=sst2['train']['label']\n","val_texts=sst2['validation']['sentence']\n","val_labels=sst2['validation']['label']"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1625397841486,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"_qz_bGusOh7V","outputId":"0f161ef2-1859-4636-8be3-5808457151a1"},"outputs":[{"data":{"text/plain":["67349"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["len(texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1625397574070,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"x0pt8hXFO0qq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":830,"status":"ok","timestamp":1625397849845,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"PJUHozzzbxg1"},"outputs":[],"source":["# I will take small portion\n","K=10000\n","train_dataset= MyDataset(tokenizer(texts[:K], truncation=True, padding=True), labels[:K])\n","val_dataset=  MyDataset(tokenizer(val_texts, truncation=True, padding=True), val_labels)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["================================================================================\n","Layer (type:depth-idx)                                  Param #\n","================================================================================\n","DistilBertForSequenceClassification                     --\n","├─DistilBertModel: 1-1                                  --\n","│    └─Embeddings: 2-1                                  --\n","│    │    └─Embedding: 3-1                              23,440,896\n","│    │    └─Embedding: 3-2                              393,216\n","│    │    └─LayerNorm: 3-3                              1,536\n","│    │    └─Dropout: 3-4                                --\n","│    └─Transformer: 2-2                                 --\n","│    │    └─ModuleList: 3-5                             42,527,232\n","├─Linear: 1-2                                           590,592\n","├─Linear: 1-3                                           1,538\n","├─Dropout: 1-4                                          --\n","================================================================================\n","Total params: 66,955,010\n","Trainable params: 66,955,010\n","Non-trainable params: 0\n","================================================================================"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# check the model size\n","from torchinfo import summary\n","summary(model)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":118203,"status":"ok","timestamp":1625397970025,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"WTTu1fmIxx8w","outputId":"9380a9fe-531a-4b77-8a16-2a3f95b9b17e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/guy/anaconda3/envs/mastrans/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["epoch 0: {'accuracy': 0.8520642201834863}\n","epoch 1: {'accuracy': 0.8979357798165137}\n","epoch 2: {'accuracy': 0.9036697247706422}\n"]}],"source":["from torch.utils.data import DataLoader\n","from transformers import  AdamW\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model.to(device)\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader =  DataLoader(val_dataset, batch_size=16, shuffle=True)\n","\n","optimizer = AdamW(model.parameters(), lr=1e-5)\n","\n","for epoch in range(3):\n","    model.train()\n","    for batch in train_loader:\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs[0]\n","        loss.backward()\n","        optimizer.step()\n","    model.eval()\n","    for batch in val_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        predictions=outputs.logits.argmax(dim=-1)  \n","        metric.add_batch(\n","                predictions=predictions,\n","                references=batch[\"labels\"],\n","            )\n","    eval_metric = metric.compute()\n","    print(f\"epoch {epoch}: {eval_metric}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"aborted","timestamp":1625397289992,"user":{"displayName":"Savas Yıldırım","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhdhYZMfq-hvK2xI7HqkzvJuCbfgFrIs4wypQEm5w=s64","userId":"10717726124681851716"},"user_tz":-180},"id":"DjrjOM3rszDB"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyM5pjZCMHxKy+F4L6/nJMEc","collapsed_sections":[],"name":"CH05b_Training_with_Naitve_PyTorch.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('mastrans')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"46ac275f730ac4cd685e340db4d97ebc86af74e2e59cec90e9c32b87f57bddf9"}}},"nbformat":4,"nbformat_minor":0}
